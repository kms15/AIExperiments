{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic experiments with GPU programming in julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is roughly based on the CUArrays package introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using Test\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise product experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance of a simple (large) vector Hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=2^28\n",
    "x = fill(2.0f0, N)\n",
    "y = fill(3.0f0, N)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single threaded on a CPU, using broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.00 GiB\n",
       "  allocs estimate:  4\n",
       "  --------------\n",
       "  minimum time:     536.383 ms (0.03% GC)\n",
       "  median time:      568.378 ms (4.89% GC)\n",
       "  mean time:        564.861 ms (5.14% GC)\n",
       "  maximum time:     596.470 ms (10.88% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test all(x .* y .== 6.0f0)\n",
    "@benchmark z = x .* y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single threaded on a CPU, using loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.00 GiB\n",
       "  allocs estimate:  2\n",
       "  --------------\n",
       "  minimum time:     539.548 ms (0.03% GC)\n",
       "  median time:      571.494 ms (4.96% GC)\n",
       "  mean time:        572.756 ms (5.27% GC)\n",
       "  maximum time:     624.088 ms (11.52% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function serial_mul(x, y)\n",
    "    z = similar(x)\n",
    "    for i in eachindex(x,y,z)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(serial_mul(x, y) .== 6.0f0)\n",
    "@benchmark z = serial_mul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi threaded on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.00 GiB\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     89.310 ms (0.23% GC)\n",
       "  median time:      124.488 ms (39.88% GC)\n",
       "  mean time:        125.584 ms (37.60% GC)\n",
       "  maximum time:     183.462 ms (48.74% GC)\n",
       "  --------------\n",
       "  samples:          40\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_mul(x, y)\n",
    "    z = similar(x)\n",
    "    Threads.@threads for i in eachindex(x,y,z)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(threaded_mul(x, y) .== 6.0f0)\n",
    "@benchmark z = threaded_mul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU with broadcasting (multithreaded and grided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays\n",
    "\n",
    "x_gpu = cufill(2.0f0, N)\n",
    "y_gpu = cufill(3.0f0, N)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.27 KiB\n",
       "  allocs estimate:  62\n",
       "  --------------\n",
       "  minimum time:     7.605 ms (0.00% GC)\n",
       "  median time:      8.732 ms (0.00% GC)\n",
       "  mean time:        48.005 ms (36.35% GC)\n",
       "  maximum time:     271.979 ms (45.28% GC)\n",
       "  --------------\n",
       "  samples:          105\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function broadcast_gpu_mul(x, y)\n",
    "    CuArrays.@sync z = x .* y\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(broadcast_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = broadcast_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single threaded on GPU (custom kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDAnative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  912 bytes\n",
       "  allocs estimate:  34\n",
       "  --------------\n",
       "  minimum time:     12.003 s (0.00% GC)\n",
       "  median time:      12.003 s (0.00% GC)\n",
       "  mean time:        12.003 s (0.00% GC)\n",
       "  maximum time:     12.003 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function serial_gpu_mul!(z, x, y)\n",
    "    for i in 1:length(x)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function serial_gpu_mul(x, y)\n",
    "    z = similar(x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda serial_gpu_mul!(z, x, y)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(serial_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = serial_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi threaded on GPU, single SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  944 bytes\n",
       "  allocs estimate:  36\n",
       "  --------------\n",
       "  minimum time:     87.581 ms (0.00% GC)\n",
       "  median time:      89.444 ms (0.00% GC)\n",
       "  mean time:        132.137 ms (16.80% GC)\n",
       "  maximum time:     389.149 ms (41.57% GC)\n",
       "  --------------\n",
       "  samples:          39\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_gpu_mul!(z, x, y)\n",
    "    index = threadIdx().x\n",
    "    stride = blockDim().x\n",
    "    \n",
    "    for i in index:stride:length(x)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function threaded_gpu_mul(x, y)\n",
    "    z = similar(x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=1024 threaded_gpu_mul!(z, x, y)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(threaded_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = threaded_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreaded and grided on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grided_gpu_mul (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grided_gpu_mul!(z, x, y)\n",
    "    index = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    \n",
    "    for i in index:stride:length(x)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function grided_gpu_mul(x, y)\n",
    "    z = similar(x)\n",
    "    numthreads = 64\n",
    "    numblocks = ceil(Int, length(x)/numthreads)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=numthreads blocks=numblocks grided_gpu_mul!(z, x, y)\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  960 bytes\n",
       "  allocs estimate:  36\n",
       "  --------------\n",
       "  minimum time:     7.633 ms (0.00% GC)\n",
       "  median time:      8.687 ms (0.00% GC)\n",
       "  mean time:        49.028 ms (37.11% GC)\n",
       "  maximum time:     299.786 ms (50.66% GC)\n",
       "  --------------\n",
       "  samples:          105\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test all(grided_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = grided_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numgpus = 10\n",
    "n = ceil(Int, N/numgpus)\n",
    "\n",
    "function devicefill(dev, value, shape)\n",
    "    device!(dev) do\n",
    "        return cufill(value, shape)\n",
    "    end\n",
    "end\n",
    "\n",
    "x_gpus = [devicefill(dev, 2.0f0, n) for dev in 0:(numgpus-1)]\n",
    "y_gpus = [devicefill(dev, 3.0f0, n) for dev in 0:(numgpus-1)]\n",
    "z_gpus = [devicefill(dev, 2.0f0, n) for dev in 0:(numgpus-1)]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001551 seconds (252 CPU allocations: 6.781 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  6.78 KiB\n",
       "  allocs estimate:  252\n",
       "  --------------\n",
       "  minimum time:     882.970 μs (0.00% GC)\n",
       "  median time:      969.710 μs (0.00% GC)\n",
       "  mean time:        991.318 μs (0.37% GC)\n",
       "  maximum time:     29.732 ms (61.92% GC)\n",
       "  --------------\n",
       "  samples:          4973\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multi_gpu_mul!(z_gpus, x_gpus, y_gpus)\n",
    "    function kernel!(z, x, y)\n",
    "        index = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "        stride = blockDim().x * gridDim().x\n",
    "    \n",
    "        for i in index:stride:length(x)\n",
    "            @inbounds z[i] = x[i] * y[i]\n",
    "        end\n",
    "        return nothing\n",
    "    end\n",
    "    \n",
    "    # launch each kernel\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            numthreads = 128\n",
    "            numblocks = ceil(Int, length(x_gpus[i])/numthreads)\n",
    "            @cuda threads=numthreads blocks=numblocks kernel!(z_gpus[i], x_gpus[i], y_gpus[i])\n",
    "        end\n",
    "    end\n",
    "    # resynchronize each device (for timing purposes)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            CuArrays.@sync 1\n",
    "        end\n",
    "    end\n",
    "    return z_gpus\n",
    "end\n",
    "\n",
    "@test all(vcat((Array.(multi_gpu_mul!(z_gpus, x_gpus, y_gpus)))...) .== 6.0f0)\n",
    "CuArrays.@time multi_gpu_mul!(z_gpus, x_gpus, y_gpus)\n",
    "@benchmark multi_gpu_mul!(z_gpus, x_gpus, y_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor contraction experiment\n",
    "\n",
    "Calculate a simple tensor contraction, i.e.\n",
    "\n",
    "$$z_{i,j,k} = \\sum_{l} x_{l,i,j} y_{l, k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "rng = MersenneTwister(1842)\n",
    "\n",
    "t1 = randn(rng, Float32, (2000, 600, 300))\n",
    "t2 = randn(rng, Float32, (2000, 300))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of single precision floating point operations involved in this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215946000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numspflops = *(size(t1)[2:end]..., size(t2)[2:end]...) * (2 * size(t1)[1] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU single threaded approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 2.1440571684936267\n",
      "trial = Trial(100.718 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     100.718 s (0.00% GC)\n",
       "  median time:      100.718 s (0.00% GC)\n",
       "  mean time:        100.718 s (0.00% GC)\n",
       "  maximum time:     100.718 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_loop!(t3, t1, t2)\n",
    "    for k in 1:size(t3)[3]\n",
    "        for j in 1:size(t3)[2]\n",
    "            for i in 1:size(t3)[1]\n",
    "                @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "\n",
    "                for l in 2:size(t1)[1]\n",
    "                    @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_loop!($t3, $t1, $t2)\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU multi-threaded approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 64.57281561846618\n",
      "trial = Trial(3.096 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  48 bytes\n",
       "  allocs estimate:  1\n",
       "  --------------\n",
       "  minimum time:     3.096 s (0.00% GC)\n",
       "  median time:      3.344 s (0.00% GC)\n",
       "  mean time:        3.344 s (0.00% GC)\n",
       "  maximum time:     3.593 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          2\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3_2 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_loop_threaded!(t3, t1, t2)\n",
    "    Threads.@threads for k in 1:size(t3)[3]\n",
    "        for j in 1:size(t3)[2]\n",
    "            for i in 1:size(t3)[1]\n",
    "                @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "\n",
    "                for l in 2:size(t1)[1]\n",
    "                    @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_loop_threaded!($t3_2, $t1, $t2)\n",
    "@test maximum(abs.(t3_2 - t3)) .< 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 188.85996765413825\n",
      "trial = Trial(1.140 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  688 bytes\n",
       "  allocs estimate:  17\n",
       "  --------------\n",
       "  minimum time:     1.140 s (0.00% GC)\n",
       "  median time:      1.143 s (0.00% GC)\n",
       "  mean time:        1.143 s (0.00% GC)\n",
       "  maximum time:     1.144 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1cu = CuArray(t1)\n",
    "t2cu = CuArray(t2)\n",
    "t3cu = CuArray{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_gpu!(t3, t1, t2)\n",
    "    function kernel!(t3, t1, t2)\n",
    "        xindex = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "        yindex = threadIdx().y + (blockIdx().y - 1)*blockDim().y\n",
    "        zindex = threadIdx().z + (blockIdx().z - 1)*blockDim().z\n",
    "        \n",
    "        xstride = blockDim().x * gridDim().x\n",
    "        ystride = blockDim().y * gridDim().y\n",
    "        zstride = blockDim().z * gridDim().z\n",
    "    \n",
    "        for k in zindex:zstride:size(t3)[3]\n",
    "            for j in yindex:ystride:size(t3)[2]\n",
    "                for i in xindex:xstride:size(t3)[1]\n",
    "                    @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "                    \n",
    "                    for l in 2:size(t1)[1]\n",
    "                        @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    numthreads = (4,4,4)\n",
    "    numblocks = map(x -> ceil(Int, x), (size(t3) ./ numthreads))\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=numthreads blocks=numblocks kernel!(t3, t1, t2)\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_gpu!($t3cu, $t1cu, $t2cu)\n",
    "@test maximum(abs.(Array(t3cu) - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function allocgpuarrays(t1, t2, numgpus)\n",
    "    t1s = Array{CuArray{Float32,3}}(undef,0)\n",
    "    t2s = Array{CuArray{Float32,2}}(undef,0)\n",
    "    t3s = Array{CuArray{Float32,3}}(undef,0)\n",
    "    slabsize = ceil(Int, size(t1)[2]/numgpus)\n",
    "    for i in 1:numgpus\n",
    "        indexstart = (i-1)*slabsize + 1\n",
    "        indexend = min(i*slabsize, size(t1)[2])\n",
    "        device!(i-1) do\n",
    "            push!(t1s, CuArray(t1[:,indexstart:indexend,:]))\n",
    "            push!(t2s, CuArray(t2))\n",
    "            push!(t3s, CuArray{Float32}(undef, indexend - indexstart + 1, size(t1)[3], size(t2)[2]))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return t1s, t2s, t3s\n",
    "end\n",
    "\n",
    "t1s, t2s, t3s = allocgpuarrays(t1, t2, numgpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function calc_t3_gpu_nosync!(t3, t1, t2)\n",
    "    function kernel!(t3, t1, t2)\n",
    "        xindex = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "        yindex = threadIdx().y + (blockIdx().y - 1)*blockDim().y\n",
    "        zindex = threadIdx().z + (blockIdx().z - 1)*blockDim().z\n",
    "        \n",
    "        xstride = blockDim().x * gridDim().x\n",
    "        ystride = blockDim().y * gridDim().y\n",
    "        zstride = blockDim().z * gridDim().z\n",
    "    \n",
    "        for k in zindex:zstride:size(t3)[3]\n",
    "            for j in yindex:ystride:size(t3)[2]\n",
    "                for i in xindex:xstride:size(t3)[1]\n",
    "                    @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "                    \n",
    "                    for l in 2:size(t1)[1]\n",
    "                        @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    numthreads = (8,4,4)\n",
    "    numblocks = map(x -> ceil(Int, x), (size(t3) ./ numthreads))\n",
    "    @cuda threads=numthreads blocks=numblocks kernel!(t3, t1, t2)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 1437.0367934782462\n",
      "trial = Trial(150.211 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  7.56 KiB\n",
       "  allocs estimate:  262\n",
       "  --------------\n",
       "  minimum time:     150.211 ms (0.00% GC)\n",
       "  median time:      150.272 ms (0.00% GC)\n",
       "  mean time:        151.097 ms (0.00% GC)\n",
       "  maximum time:     162.362 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          34\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calc_t3_gpus!(t3s, t1s, t2s)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            calc_t3_gpu_nosync!(t3s[i], t1s[i], t2s[i])\n",
    "        end\n",
    "    end\n",
    "    # resynchronize each device (for timing purposes)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            CuArrays.@sync 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_gpus!($t3s, $t1s, $t2s)\n",
    "@test maximum(abs.(vcat(map(Array,t3s)...) - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorOperations library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 376.0833392710437\n",
      "trial = Trial(556.707 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  128 bytes\n",
       "  allocs estimate:  2\n",
       "  --------------\n",
       "  minimum time:     556.707 ms (0.00% GC)\n",
       "  median time:      574.197 ms (0.00% GC)\n",
       "  mean time:        574.483 ms (0.00% GC)\n",
       "  maximum time:     597.812 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TensorOperations\n",
    "\n",
    "t3_3 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_library!(t3, t1, t2)\n",
    "    @tensor t3[i,j,k] = t1[l,i,j] * t2[l,k]\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_library!($t3_3, $t1, $t2)\n",
    "@test maximum(abs.(t3_3 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearAlgebra library with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 369.692449782154\n",
      "trial = Trial(569.559 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  192 bytes\n",
       "  allocs estimate:  4\n",
       "  --------------\n",
       "  minimum time:     569.559 ms (0.00% GC)\n",
       "  median time:      584.123 ms (0.00% GC)\n",
       "  mean time:        589.770 ms (0.00% GC)\n",
       "  maximum time:     626.568 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "t3_4 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_matmul!(t3, t1, t2)\n",
    "    t1_2d_t = transpose(reshape(t1, size(t1)[1], size(t1)[2]*size(t1)[3]))\n",
    "    t3_2d = reshape(t3, size(t3)[1]*size(t3)[2], size(t3)[3])\n",
    "    mul!(t3_2d, t1_2d_t, t2)\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_matmul!($t3_4, $t1, $t2)\n",
    "@test maximum(abs.(t3_4 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearAlgebra library with single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 11453.562198078747\n",
      "trial = Trial(17.813 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  304 bytes\n",
       "  allocs estimate:  6\n",
       "  --------------\n",
       "  minimum time:     17.813 ms (0.00% GC)\n",
       "  median time:      18.854 ms (0.00% GC)\n",
       "  mean time:        18.803 ms (0.00% GC)\n",
       "  maximum time:     19.848 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          266\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_2d_t_cu = CuArray(Array(transpose(reshape(t1, size(t1)[1], size(t1)[2]*size(t1)[3]))))\n",
    "t2cu = CuArray(t2)\n",
    "t3_2d_cu = CuArray{Float32}(undef, size(t1)[2] * size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_matmul_gpu!(t3_2d, t1_2d_t, t2)\n",
    "    CuArrays.@sync begin\n",
    "        LinearAlgebra.mul!(t3_2d, t1_2d_t, t2)\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_matmul_gpu!($t3_2d_cu, $t1_2d_t_cu, $t2cu)\n",
    "t3_5 = reshape(Array(t3_2d_cu), size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "@test maximum(abs.(t3_5 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearAlgebra library with multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "function allocgpuarrays_2d(t1, t2, numgpus)\n",
    "    t1_2d_t = transpose(reshape(t1, size(t1)[1], size(t1)[2]*size(t1)[3]))\n",
    "\n",
    "    t1s_2d = Array{CuArray{Float32,2}}(undef,0)\n",
    "    t2s = Array{CuArray{Float32,2}}(undef,0)\n",
    "    t3s_2d = Array{CuArray{Float32,2}}(undef,0)\n",
    "    \n",
    "    slabsize = ceil(Int, size(t1_2d_t)[1]/numgpus)\n",
    "    \n",
    "    for i in 1:numgpus\n",
    "        indexstart = (i-1)*slabsize + 1\n",
    "        indexend = min(i*slabsize, size(t1_2d_t)[1])\n",
    "        device!(i-1) do\n",
    "            push!(t1s_2d, CuArray(t1_2d_t[indexstart:indexend,:]))\n",
    "            push!(t2s, CuArray(t2))\n",
    "            push!(t3s_2d, CuArray{Float32}(undef, indexend - indexstart + 1, size(t2)[2]))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return t1s_2d, t2s, t3s_2d\n",
    "end\n",
    "\n",
    "t1s_2d_t, t2s, t3s_2d = allocgpuarrays_2d(t1, t2, numgpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 89611.1397901579\n",
      "trial = Trial(2.127 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  5.53 KiB\n",
       "  allocs estimate:  182\n",
       "  --------------\n",
       "  minimum time:     2.127 ms (0.00% GC)\n",
       "  median time:      2.410 ms (0.00% GC)\n",
       "  mean time:        2.408 ms (0.00% GC)\n",
       "  maximum time:     8.526 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          2064\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calc_t3_matmul_gpus!(t3s_2d, t1s_2d_t, t2s)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            LinearAlgebra.mul!(t3s_2d[i], t1s_2d_t[i], t2s[i])\n",
    "        end\n",
    "    end\n",
    "    # resynchronize each device (for timing purposes)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            CuArrays.@sync 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_matmul_gpus!($t3s_2d, $t1s_2d_t, $t2s)\n",
    "t3_6 = reshape(vcat(map(Array,t3s_2d)...), size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "@test maximum(abs.(t3_6 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (44 threads) 1.0.3",
   "language": "julia",
   "name": "julia-(44-threads)-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
