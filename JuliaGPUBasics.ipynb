{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic experiments with GPU programming in julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is roughly based on the CUArrays package introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using Test\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise product experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance of a simple (large) vector Hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=2^28\n",
    "x = fill(2.0f0, N)\n",
    "y = fill(3.0f0, N)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single threaded on a CPU, using broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.00 GiB\n",
       "  allocs estimate:  4\n",
       "  --------------\n",
       "  minimum time:     537.403 ms (0.03% GC)\n",
       "  median time:      565.518 ms (4.75% GC)\n",
       "  mean time:        565.881 ms (5.18% GC)\n",
       "  maximum time:     607.860 ms (11.14% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test all(x .* y .== 6.0f0)\n",
    "@benchmark z = x .* y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single threaded on a CPU, using loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.00 GiB\n",
       "  allocs estimate:  2\n",
       "  --------------\n",
       "  minimum time:     532.815 ms (0.03% GC)\n",
       "  median time:      562.928 ms (4.81% GC)\n",
       "  mean time:        563.425 ms (5.24% GC)\n",
       "  maximum time:     607.163 ms (11.23% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function serial_mul(x, y)\n",
    "    z = similar(x)\n",
    "    for i in eachindex(x,y,z)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(serial_mul(x, y) .== 6.0f0)\n",
    "@benchmark z = serial_mul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi threaded on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.00 GiB\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     86.265 ms (12.86% GC)\n",
       "  median time:      119.062 ms (35.40% GC)\n",
       "  mean time:        121.156 ms (34.07% GC)\n",
       "  maximum time:     190.826 ms (43.43% GC)\n",
       "  --------------\n",
       "  samples:          42\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_mul(x, y)\n",
    "    z = similar(x)\n",
    "    Threads.@threads for i in eachindex(x,y,z)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(threaded_mul(x, y) .== 6.0f0)\n",
    "@benchmark z = threaded_mul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU with broadcasting (multithreaded and grided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays\n",
    "\n",
    "x_gpu = cufill(2.0f0, N)\n",
    "y_gpu = cufill(3.0f0, N)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.27 KiB\n",
       "  allocs estimate:  62\n",
       "  --------------\n",
       "  minimum time:     7.666 ms (0.00% GC)\n",
       "  median time:      8.759 ms (0.00% GC)\n",
       "  mean time:        48.827 ms (37.97% GC)\n",
       "  maximum time:     272.222 ms (47.15% GC)\n",
       "  --------------\n",
       "  samples:          105\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function broadcast_gpu_mul(x, y)\n",
    "    CuArrays.@sync z = x .* y\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(broadcast_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = broadcast_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single threaded on GPU (custom kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDAnative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  912 bytes\n",
       "  allocs estimate:  34\n",
       "  --------------\n",
       "  minimum time:     12.004 s (0.00% GC)\n",
       "  median time:      12.004 s (0.00% GC)\n",
       "  mean time:        12.004 s (0.00% GC)\n",
       "  maximum time:     12.004 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function serial_gpu_mul!(z, x, y)\n",
    "    for i in 1:length(x)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function serial_gpu_mul(x, y)\n",
    "    z = similar(x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda serial_gpu_mul!(z, x, y)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(serial_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = serial_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi threaded on GPU, single SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  944 bytes\n",
       "  allocs estimate:  36\n",
       "  --------------\n",
       "  minimum time:     87.682 ms (0.00% GC)\n",
       "  median time:      89.418 ms (0.00% GC)\n",
       "  mean time:        130.864 ms (15.73% GC)\n",
       "  maximum time:     381.590 ms (40.54% GC)\n",
       "  --------------\n",
       "  samples:          39\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_gpu_mul!(z, x, y)\n",
    "    index = threadIdx().x\n",
    "    stride = blockDim().x\n",
    "    \n",
    "    for i in index:stride:length(x)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function threaded_gpu_mul(x, y)\n",
    "    z = similar(x)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=1024 threaded_gpu_mul!(z, x, y)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "\n",
    "@test all(threaded_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = threaded_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreaded and grided on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grided_gpu_mul (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grided_gpu_mul!(z, x, y)\n",
    "    index = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "    stride = blockDim().x * gridDim().x\n",
    "    \n",
    "    for i in index:stride:length(x)\n",
    "        @inbounds z[i] = x[i] * y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function grided_gpu_mul(x, y)\n",
    "    z = similar(x)\n",
    "    numthreads = 64\n",
    "    numblocks = ceil(Int, length(x)/numthreads)\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=numthreads blocks=numblocks grided_gpu_mul!(z, x, y)\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  960 bytes\n",
       "  allocs estimate:  36\n",
       "  --------------\n",
       "  minimum time:     7.398 ms (0.00% GC)\n",
       "  median time:      8.396 ms (0.00% GC)\n",
       "  mean time:        46.954 ms (35.94% GC)\n",
       "  maximum time:     295.116 ms (49.44% GC)\n",
       "  --------------\n",
       "  samples:          111\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test all(grided_gpu_mul(x_gpu, y_gpu) .== 6.0f0)\n",
    "@benchmark z_gpu = grided_gpu_mul(x_gpu, y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numgpus = 10\n",
    "n = ceil(Int, N/numgpus)\n",
    "\n",
    "function devicefill(dev, value, shape)\n",
    "    device!(dev) do\n",
    "        return cufill(value, shape)\n",
    "    end\n",
    "end\n",
    "\n",
    "x_gpus = [devicefill(dev, 2.0f0, n) for dev in 0:(numgpus-1)]\n",
    "y_gpus = [devicefill(dev, 3.0f0, n) for dev in 0:(numgpus-1)]\n",
    "z_gpus = [devicefill(dev, 2.0f0, n) for dev in 0:(numgpus-1)]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001085 seconds (252 CPU allocations: 6.781 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  6.78 KiB\n",
       "  allocs estimate:  252\n",
       "  --------------\n",
       "  minimum time:     838.764 μs (0.00% GC)\n",
       "  median time:      977.276 μs (0.00% GC)\n",
       "  mean time:        992.443 μs (0.35% GC)\n",
       "  maximum time:     28.716 ms (60.22% GC)\n",
       "  --------------\n",
       "  samples:          4965\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multi_gpu_mul!(z_gpus, x_gpus, y_gpus)\n",
    "    function kernel!(z, x, y)\n",
    "        index = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "        stride = blockDim().x * gridDim().x\n",
    "    \n",
    "        for i in index:stride:length(x)\n",
    "            @inbounds z[i] = x[i] * y[i]\n",
    "        end\n",
    "        return nothing\n",
    "    end\n",
    "    \n",
    "    # launch each kernel\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            numthreads = 128\n",
    "            numblocks = ceil(Int, length(x_gpus[i])/numthreads)\n",
    "            @cuda threads=numthreads blocks=numblocks kernel!(z_gpus[i], x_gpus[i], y_gpus[i])\n",
    "        end\n",
    "    end\n",
    "    # resynchronize each device (for timing purposes)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            CuArrays.@sync 1\n",
    "        end\n",
    "    end\n",
    "    return z_gpus\n",
    "end\n",
    "\n",
    "@test all(vcat((Array.(multi_gpu_mul!(z_gpus, x_gpus, y_gpus)))...) .== 6.0f0)\n",
    "CuArrays.@time multi_gpu_mul!(z_gpus, x_gpus, y_gpus)\n",
    "@benchmark multi_gpu_mul!(z_gpus, x_gpus, y_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor contraction experiment\n",
    "\n",
    "Calculate a simple tensor contraction, i.e.\n",
    "\n",
    "$$z_{i,j,k} = \\sum_{l} x_{l,i,j} y_{l, k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "rng = MersenneTwister(1842)\n",
    "\n",
    "t1 = randn(rng, Float32, (2000, 600, 300))\n",
    "t2 = randn(rng, Float32, (2000, 300))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of single precision floating point operations involved in this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215946000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numspflops = *(size(t1)[2:end]..., size(t2)[2:end]...) * (2 * size(t1)[1] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU single threaded approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 2.19570752429342\n",
      "trial = Trial(98.349 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     98.349 s (0.00% GC)\n",
       "  median time:      98.349 s (0.00% GC)\n",
       "  mean time:        98.349 s (0.00% GC)\n",
       "  maximum time:     98.349 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_loop!(t3, t1, t2)\n",
    "    for k in 1:size(t3)[3]\n",
    "        for j in 1:size(t3)[2]\n",
    "            for i in 1:size(t3)[1]\n",
    "                @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "\n",
    "                for l in 2:size(t1)[1]\n",
    "                    @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_loop!($t3, $t1, $t2)\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU multi-threaded approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 66.07622605099742\n",
      "trial = Trial(3.172 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  48 bytes\n",
       "  allocs estimate:  1\n",
       "  --------------\n",
       "  minimum time:     3.172 s (0.00% GC)\n",
       "  median time:      3.268 s (0.00% GC)\n",
       "  mean time:        3.268 s (0.00% GC)\n",
       "  maximum time:     3.364 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          2\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3_2 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_loop_threaded!(t3, t1, t2)\n",
    "    Threads.@threads for k in 1:size(t3)[3]\n",
    "        for j in 1:size(t3)[2]\n",
    "            for i in 1:size(t3)[1]\n",
    "                @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "\n",
    "                for l in 2:size(t1)[1]\n",
    "                    @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_loop_threaded!($t3_2, $t1, $t2)\n",
    "@test maximum(abs.(t3_2 - t3)) .< 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 188.65600106701163\n",
      "trial = Trial(1.142 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  688 bytes\n",
       "  allocs estimate:  17\n",
       "  --------------\n",
       "  minimum time:     1.142 s (0.00% GC)\n",
       "  median time:      1.145 s (0.00% GC)\n",
       "  mean time:        1.144 s (0.00% GC)\n",
       "  maximum time:     1.146 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1cu = CuArray(t1)\n",
    "t2cu = CuArray(t2)\n",
    "t3cu = CuArray{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_gpu!(t3, t1, t2)\n",
    "    function kernel!(t3, t1, t2)\n",
    "        xindex = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "        yindex = threadIdx().y + (blockIdx().y - 1)*blockDim().y\n",
    "        zindex = threadIdx().z + (blockIdx().z - 1)*blockDim().z\n",
    "        \n",
    "        xstride = blockDim().x * gridDim().x\n",
    "        ystride = blockDim().y * gridDim().y\n",
    "        zstride = blockDim().z * gridDim().z\n",
    "    \n",
    "        for k in zindex:zstride:size(t3)[3]\n",
    "            for j in yindex:ystride:size(t3)[2]\n",
    "                for i in xindex:xstride:size(t3)[1]\n",
    "                    @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "                    \n",
    "                    for l in 2:size(t1)[1]\n",
    "                        @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    numthreads = (4,4,4)\n",
    "    numblocks = map(x -> ceil(Int, x), (size(t3) ./ numthreads))\n",
    "    CuArrays.@sync begin\n",
    "        @cuda threads=numthreads blocks=numblocks kernel!(t3, t1, t2)\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_gpu!($t3cu, $t1cu, $t2cu)\n",
    "@test maximum(abs.(Array(t3cu) - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function allocgpuarrays(t1, t2, numgpus)\n",
    "    t1s = Array{CuArray{Float32,3}}(undef,0)\n",
    "    t2s = Array{CuArray{Float32,2}}(undef,0)\n",
    "    t3s = Array{CuArray{Float32,3}}(undef,0)\n",
    "    slabsize = ceil(Int, size(t1)[2]/numgpus)\n",
    "    for i in 1:numgpus\n",
    "        indexstart = (i-1)*slabsize + 1\n",
    "        indexend = min(i*slabsize, size(t1)[2])\n",
    "        device!(i-1) do\n",
    "            push!(t1s, CuArray(t1[:,indexstart:indexend,:]))\n",
    "            push!(t2s, CuArray(t2))\n",
    "            push!(t3s, CuArray{Float32}(undef, indexend - indexstart + 1, size(t1)[3], size(t2)[2]))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return t1s, t2s, t3s\n",
    "end\n",
    "\n",
    "t1s, t2s, t3s = allocgpuarrays(t1, t2, numgpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function calc_t3_gpu_nosync!(t3, t1, t2)\n",
    "    function kernel!(t3, t1, t2)\n",
    "        xindex = threadIdx().x + (blockIdx().x - 1)*blockDim().x\n",
    "        yindex = threadIdx().y + (blockIdx().y - 1)*blockDim().y\n",
    "        zindex = threadIdx().z + (blockIdx().z - 1)*blockDim().z\n",
    "        \n",
    "        xstride = blockDim().x * gridDim().x\n",
    "        ystride = blockDim().y * gridDim().y\n",
    "        zstride = blockDim().z * gridDim().z\n",
    "    \n",
    "        for k in zindex:zstride:size(t3)[3]\n",
    "            for j in yindex:ystride:size(t3)[2]\n",
    "                for i in xindex:xstride:size(t3)[1]\n",
    "                    @inbounds t3[i,j,k] = t1[1,i,j] * t2[1,k]\n",
    "                    \n",
    "                    for l in 2:size(t1)[1]\n",
    "                        @inbounds t3[i,j,k] += t1[l,i,j] * t2[l,k]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    numthreads = (8,4,4)\n",
    "    numblocks = map(x -> ceil(Int, x), (size(t3) ./ numthreads))\n",
    "    @cuda threads=numthreads blocks=numblocks kernel!(t3, t1, t2)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 1423.0433677583312\n",
      "trial = Trial(150.753 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  7.56 KiB\n",
       "  allocs estimate:  262\n",
       "  --------------\n",
       "  minimum time:     150.753 ms (0.00% GC)\n",
       "  median time:      151.749 ms (0.00% GC)\n",
       "  mean time:        151.752 ms (0.00% GC)\n",
       "  maximum time:     152.874 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          33\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calc_t3_gpus!(t3s, t1s, t2s)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            calc_t3_gpu_nosync!(t3s[i], t1s[i], t2s[i])\n",
    "        end\n",
    "    end\n",
    "    # resynchronize each device (for timing purposes)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            CuArrays.@sync 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_gpus!($t3s, $t1s, $t2s)\n",
    "@test maximum(abs.(vcat(map(Array,t3s)...) - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorOperations library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LinearAlgebra.BLAS\n",
    "BLAS.set_num_threads(Sys.CPU_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 791.9345819920223\n",
      "trial = Trial(259.649 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  128 bytes\n",
       "  allocs estimate:  2\n",
       "  --------------\n",
       "  minimum time:     259.649 ms (0.00% GC)\n",
       "  median time:      272.682 ms (0.00% GC)\n",
       "  mean time:        299.894 ms (0.00% GC)\n",
       "  maximum time:     370.240 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          17\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TensorOperations\n",
    "\n",
    "t3_3 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_library!(t3, t1, t2)\n",
    "    @tensor t3[i,j,k] = t1[l,i,j] * t2[l,k]\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_library!($t3_3, $t1, $t2)\n",
    "@test maximum(abs.(t3_3 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearAlgebra library with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 819.8135214851668\n",
      "trial = Trial(252.848 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  192 bytes\n",
       "  allocs estimate:  4\n",
       "  --------------\n",
       "  minimum time:     252.848 ms (0.00% GC)\n",
       "  median time:      263.409 ms (0.00% GC)\n",
       "  mean time:        282.770 ms (0.00% GC)\n",
       "  maximum time:     523.194 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          18\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "t3_4 = Array{Float32}(undef, size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_matmul!(t3, t1, t2)\n",
    "    t1_2d_t = transpose(reshape(t1, size(t1)[1], size(t1)[2]*size(t1)[3]))\n",
    "    t3_2d = reshape(t3, size(t3)[1]*size(t3)[2], size(t3)[3])\n",
    "    mul!(t3_2d, t1_2d_t, t2)\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_matmul!($t3_4, $t1, $t2)\n",
    "@test maximum(abs.(t3_4 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearAlgebra library with a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 11540.25713847197\n",
      "trial = Trial(17.653 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  304 bytes\n",
       "  allocs estimate:  6\n",
       "  --------------\n",
       "  minimum time:     17.653 ms (0.00% GC)\n",
       "  median time:      18.712 ms (0.00% GC)\n",
       "  mean time:        18.737 ms (0.00% GC)\n",
       "  maximum time:     19.850 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          267\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_2d_t_cu = CuArray(Array(transpose(reshape(t1, size(t1)[1], size(t1)[2]*size(t1)[3]))))\n",
    "t2cu = CuArray(t2)\n",
    "t3_2d_cu = CuArray{Float32}(undef, size(t1)[2] * size(t1)[3], size(t2)[2])\n",
    "\n",
    "function calc_t3_matmul_gpu!(t3_2d, t1_2d_t, t2)\n",
    "    CuArrays.@sync begin\n",
    "        LinearAlgebra.mul!(t3_2d, t1_2d_t, t2)\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_matmul_gpu!($t3_2d_cu, $t1_2d_t_cu, $t2cu)\n",
    "t3_5 = reshape(Array(t3_2d_cu), size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "@test maximum(abs.(t3_5 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearAlgebra library with multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "function allocgpuarrays_2d(t1, t2, numgpus)\n",
    "    t1_2d_t = transpose(reshape(t1, size(t1)[1], size(t1)[2]*size(t1)[3]))\n",
    "\n",
    "    t1s_2d = Array{CuArray{Float32,2}}(undef,0)\n",
    "    t2s = Array{CuArray{Float32,2}}(undef,0)\n",
    "    t3s_2d = Array{CuArray{Float32,2}}(undef,0)\n",
    "    \n",
    "    slabsize = ceil(Int, size(t1_2d_t)[1]/numgpus)\n",
    "    \n",
    "    for i in 1:numgpus\n",
    "        indexstart = (i-1)*slabsize + 1\n",
    "        indexend = min(i*slabsize, size(t1_2d_t)[1])\n",
    "        device!(i-1) do\n",
    "            push!(t1s_2d, CuArray(t1_2d_t[indexstart:indexend,:]))\n",
    "            push!(t2s, CuArray(t2))\n",
    "            push!(t3s_2d, CuArray{Float32}(undef, indexend - indexstart + 1, size(t2)[2]))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return t1s_2d, t2s, t3s_2d\n",
    "end\n",
    "\n",
    "t1s_2d_t, t2s, t3s_2d = allocgpuarrays_2d(t1, t2, numgpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFLOPS (SP): 90839.05163030619\n",
      "trial = Trial(2.129 ms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  5.53 KiB\n",
       "  allocs estimate:  182\n",
       "  --------------\n",
       "  minimum time:     2.129 ms (0.00% GC)\n",
       "  median time:      2.377 ms (0.00% GC)\n",
       "  mean time:        2.397 ms (0.00% GC)\n",
       "  maximum time:     7.744 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          2074\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calc_t3_matmul_gpus!(t3s_2d, t1s_2d_t, t2s)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            LinearAlgebra.mul!(t3s_2d[i], t1s_2d_t[i], t2s[i])\n",
    "        end\n",
    "    end\n",
    "    # resynchronize each device (for timing purposes)\n",
    "    for i in 1:numgpus\n",
    "        device!(i-1) do\n",
    "            CuArrays.@sync 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "trial = @benchmark calc_t3_matmul_gpus!($t3s_2d, $t1s_2d_t, $t2s)\n",
    "t3_6 = reshape(vcat(map(Array,t3s_2d)...), size(t1)[2], size(t1)[3], size(t2)[2])\n",
    "@test maximum(abs.(t3_6 - t3)) < 2e-5*√(size(t1)[1]) # confirm results match the single threaded results\n",
    "print(\"GFLOPS (SP): $(numspflops/median(trial).time)\\n\")\n",
    "@show trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (44 threads) 1.0.3",
   "language": "julia",
   "name": "julia-(44-threads)-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
